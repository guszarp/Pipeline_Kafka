ETL de Vendas e Dashboard de KPIs em Tempo Real
Este projeto apresenta um pipeline ETL completo ‚Äî desde a gera√ß√£o de dados at√© a exibi√ß√£o de KPIs em tempo real ‚Äî com foco em pr√°ticas modernas de engenharia de dados. Ele simula um fluxo de vendas utilizando armazenamento em nuvem, monitoramento via Kafka e visualiza√ß√£o interativa. Neste documento, voc√™ encontrar√° detalhes sobre a arquitetura do sistema, os principais componentes envolvidos e instru√ß√µes para executar o projeto localmente ou com Docker.

üìö Sum√°rio
Vis√£o Geral

Tecnologias e Bibliotecas

Arquitetura

Configura√ß√£o do Ambiente

Fluxo de Dados

KPIs Monitorados

Vis√£o Geral
A proposta do projeto √© simular a rotina de gera√ß√£o de relat√≥rios de vendas di√°rios. Um arquivo CSV com dados simulados √© criado e enviado a um bucket S3 da AWS, que atua como ponto de entrada para o pipeline. Um processo autom√°tico monitora esse bucket e, ao detectar novos arquivos, inicia o pipeline ETL que:

Extrai os dados do bucket S3;

Transforma os dados com limpeza e valida√ß√£o usando pandas e pydantic;

Carrega os dados em um banco de dados PostgreSQL.

Esses dados s√£o ent√£o consumidos por uma aplica√ß√£o Streamlit, que exibe indicadores de desempenho de vendas em tempo real.

Tecnologias e Bibliotecas
Infraestrutura
AWS S3 ‚Äì Armazena os arquivos CSV de vendas, servindo como fonte do ETL

PostgreSQL ‚Äì Banco de dados relacional onde os dados transformados s√£o armazenados

Docker ‚Äì Facilita a cria√ß√£o e execu√ß√£o de ambientes isolados

Kafka ‚Äì Respons√°vel por acionar e monitorar o pipeline ETL por meio de mensagens

Ferramentas de Apoio
DBeaver ‚Äì Interface de gerenciamento e consulta ao banco de dados

Principais Bibliotecas Python
pandas ‚Äì Manipula√ß√£o e transforma√ß√£o de dados

boto3 ‚Äì SDK para comunica√ß√£o com servi√ßos AWS (como S3)

Faker ‚Äì Gera√ß√£o de dados de vendas fict√≠cios

pydantic ‚Äì Valida√ß√£o e integridade dos dados em cada etapa do pipeline

sqlalchemy ‚Äì ORM para integra√ß√£o com o PostgreSQL

streamlit ‚Äì Framework para cria√ß√£o de dashboards interativos em tempo real

confluent_kafka ‚Äì Biblioteca de integra√ß√£o com Kafka

Arquitetura
A arquitetura √© modular e bem definida, permitindo controle total do fluxo de dados:

Gera√ß√£o de Dados: O script csv_generator.py cria arquivos com dados de vendas simulados.

Armazenamento e Detec√ß√£o: Os arquivos s√£o enviados para o S3 e monitorados por Kafka.

Pipeline ETL:

Extra√ß√£o: Download do CSV via boto3.

Transforma√ß√£o: Limpeza e valida√ß√£o com pandas e pydantic.

Carga: Inser√ß√£o no PostgreSQL usando sqlalchemy.

Dashboard: Aplica√ß√£o em Streamlit que consome os dados do PostgreSQL e apresenta KPIs atualizados em tempo real.

Configura√ß√£o do Ambiente
Requisitos
Docker

AWS CLI configurado

PostgreSQL

Kafka

Passos para Execu√ß√£o
Clone o reposit√≥rio:

bash
Copiar
Editar
git clone git@github.com/caio-moliveira/sales-pipeline-project.git
cd sales-pipeline-project
Crie o arquivo .env com as configura√ß√µes necess√°rias para PostgreSQL, AWS e Kafka.

Inicie os servi√ßos com Docker:

bash
Copiar
Editar
docker-compose up --build
Vari√°veis de Ambiente
As seguintes vari√°veis devem ser definidas no arquivo .env na raiz do projeto:

AWS
env
Copiar
Editar
AWS_ACCESS_KEY_ID=sua_chave_de_acesso
AWS_SECRET_ACCESS_KEY=sua_chave_secreta
AWS_REGION=sua_regiao (ex: us-east-1)
BUCKET_NAME=nome_do_bucket
PostgreSQL
env
Copiar
Editar
POSTGRES_USER=usuario
POSTGRES_PASSWORD=senha
POSTGRES_HOST=host
POSTGRES_DB=nome_do_banco
Kafka
env
Copiar
Editar
BOOTSTRAP_SERVERS=host_do_kafka
SASL_USERNAME=usuario_kafka
SASL_PASSWORD=senha_kafka
CLIENT_ID=id_cliente_kafka
‚ö†Ô∏è Importante: adicione o .env no .gitignore para evitar o versionamento de dados sens√≠veis.

Fluxo de Dados
Gera√ß√£o de Dados

Com Faker, um arquivo CSV com vendas fict√≠cias √© gerado e armazenado no S3.

Extra√ß√£o

O script detecta o novo arquivo e o baixa utilizando boto3.

Transforma√ß√£o

pandas organiza os dados e pydantic garante a integridade dos registros.

Carga

Os dados validados s√£o inseridos no PostgreSQL via sqlalchemy.

Visualiza√ß√£o

A aplica√ß√£o em Streamlit se conecta ao banco e exibe os KPIs em tempo real.

KPIs Monitorados
O dashboard mostra os seguintes indicadores de desempenho:

Vendas Totais ‚Äì Soma do total de vendas no dia

Valor M√©dio por Transa√ß√£o ‚Äì Valor m√©dio por venda

Produtos Mais Vendidos ‚Äì Lista dos produtos com maior volume de vendas

Vendas por Categoria ‚Äì Distribui√ß√£o das vendas por categoria de produto

Tend√™ncia de Vendas ‚Äì Gr√°fico em tempo real da evolu√ß√£o das vendas
